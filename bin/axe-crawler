#!/usr/bin/env node
(function (global, factory) {
	typeof exports === 'object' && typeof module !== 'undefined' ? factory(require('validator'), require('selenium-webdriver'), require('selenium-webdriver/chrome'), require('axe-webdriverjs'), require('axios'), require('cheerio')) :
	typeof define === 'function' && define.amd ? define(['validator', 'selenium-webdriver', 'selenium-webdriver/chrome', 'axe-webdriverjs', 'axios', 'cheerio'], factory) :
	(factory(global.validator,global.webDriver,global.chromeDriver,global.axeBuilder,global.axios,global.cheerio));
}(this, (function (validator,webDriver,chromeDriver,axeBuilder,axios,cheerio) { 'use strict';

webDriver = webDriver && webDriver.hasOwnProperty('default') ? webDriver['default'] : webDriver;
chromeDriver = chromeDriver && chromeDriver.hasOwnProperty('default') ? chromeDriver['default'] : chromeDriver;
axeBuilder = axeBuilder && axeBuilder.hasOwnProperty('default') ? axeBuilder['default'] : axeBuilder;
axios = axios && axios.hasOwnProperty('default') ? axios['default'] : axios;
cheerio = cheerio && cheerio.hasOwnProperty('default') ? cheerio['default'] : cheerio;

/**
 * @fileOverview Utility functions for axe-crawler
 * @name util.js
 * @author Tyler Collins
 * @license MIT
 */

/**
 * notMedia - function to filter Wordpress media urls from list of urls
 *
 * @param {string} url
 */
function notMedia(url) {
  return !/(uploads\/\d{4}\/\d{2}\/)/.test(url) && !/attachment_id/.test(url) && !/\.(exe|wmv|avi|flv|mov|mkv|mp..?|swf|ra.?|rm|as.|m4[av]|smi.?|doc|docx|ppt|pptx|pps|ppsx|jpg|png|gif|pdf)$/.test(url);
}

/**
 * isDoc - function to filter out non documents and leave only document links
 *
 * @export
 * @param {any} url
 * @returns
 */


/**
 * matchDomain - returns a function to filter urls not matching domain
 *
 * @param {string} domain
 */
function matchDomain(domain) {
  return url => new RegExp(domain).test(url) || /^\/\w+/.test(url);
}

/**
 * @fileOverview Polyfills for axe-crawler
 * @name polyfills.js
 * @author Tyler Collins
 * @license MIT
 */

/**
 * polyfills - apply polyfills for needed functionality
 *
 */
function polyfills() {
  const reduce = Function.bind.call(Function.call, Array.prototype.reduce);
  const isEnumerable = Function.bind.call(Function.call, Object.prototype.propertyIsEnumerable);
  const concat = Function.bind.call(Function.call, Array.prototype.concat);
  const keys = Reflect.ownKeys;

  Set.prototype.difference = function (setB) {
    const difference = new Set(this);
    for (const elem of setB) {
      difference.delete(elem);
    }
    return difference;
  };

  if (!Object.values) {
    Object.values = function values(O) {
      return reduce(keys(O), (v, k) => concat(v, typeof k === 'string' && isEnumerable(O, k) ? [O[k]] : []), []);
    };
  }

  if (!Object.entries) {
    Object.entries = function entries(O) {
      return reduce(keys(O), (e, k) => concat(e, typeof k === 'string' && isEnumerable(O, k) ? [[k, O[k]]] : []), []);
    };
  }
}

/**
 * @fileOverview Generate output reports from axe-crawler data
 * @name output.js
 * @author Tyler Collins
 * @license MIT
 */

const fs = require('fs');
const marked = require('marked');
const escape = require('escape-html');

function outputToJSON(file, reports) {
  const formattedJSON = JSON.stringify(reports, null, 2);
  fs.writeFile(file, formattedJSON);
}

/**
 * buildHTMLReports - function to generate html version of JSON report
 *
 * @param {object} reports
 */
function outputToHTML(file, reports) {
  let head = '<!doctype html> <html lang="en"><head>';
  head += `<title>aXe Accessibility Engine Report${new Date().toDateString()}</title>`;
  head += '<meta name="viewport" content="width=device-width, initial-scale=1">';
  head += '<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.8.0/github-markdown.css">';
  head += '<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">';
  head += '<style> h1 { text-align: center; } ol > li { padding-bottom: 15px; font-weight: 500 } ul > li { font-weight: 400; font-size: 12px; } ol > li > span { font-weight: 400; }</style>';

  let body = '</head><body><div class="container"><div class="row"><div class="col-xs-12">';
  body += marked(`# aXe Accessibility Engine Report ${new Date().toDateString()}`);
  body += `${marked('This report should is not a complete accessibility audit.  This report only documents those accessibility features tested by the axe-core library, and should not be considered exhaustive of the possible accessibility issues a website may have.  More information regarding the features tested by the axe-core library may be found at [axe-core.org](https://axe-core.org/)')}</div></div>`;
  body += '<div class="row"><div class="col-xs-12">';

  //   console.log(reports);

  function countViolations(report) {
    return Object.entries(report.violations).reduce((sum, [viewName, result]) => sum + result.length, 0);
  }

  function countPasses(report) {
    return Object.entries(report.passes).reduce((sum, [viewName, result]) => sum + result.length, 0);
  }

  function writeNodeMessages(list, {
    html,
    any,
    all,
    none
  }) {
    /* eslint-disable no-param-reassign */
    list += `<li>${escape(html)}`;
    any.forEach(({
      message
    }) => {
      list += `<br />${escape(message)}`;
    });
    all.forEach(({
      message
    }) => {
      list += `<br />${escape(message)}`;
    });
    none.forEach(({
      message
    }) => {
      list += `<br />${escape(message)}`;
    });
    list += '</li>';
    return list;
    /* eslint-enable no-param-reassign */
  }
  function printViewCounts(body, report, reportType) {
    body += '<ul>';
    Object.entries(report[reportType]).forEach(([view, results]) => {
      body += `<li style="list-style: none; height: 15px;"><div class="col-xs-2">${view}:</div><div class="col-xs-10"> ${results.length} ${reportType}</div></li>`;
    });
    return body += '</ul>';
  }

  function printViolation(list, view) {
    return ({ impact, description, nodes }) => {
      list += `<li>${impact.toUpperCase()}: ${escape(description)}<br />`;
      list += '';
      list += `<span>${view.toUpperCase()} Affected Nodes: </span><ul>`;
      list += nodes.reduce(writeNodeMessages, '');
      list += '</ul></li>';
    };
  }

  try {
    body += `${marked(`## Summary of Violations: ${Object.entries(reports).reduce((total, [url, report]) => total + countViolations(report), 0)} total violations`)}</div></div>`;
  } catch (err) {
    console.log('Error:', err);
    body += marked('## Summary of Violations');
  }
  body += '<div class="row"><div class="col-xs-12">';
  Object.entries(reports).forEach(([url, report]) => {
    const violationCount = countViolations(report);

    body += marked(`### ${escape(url)} ${violationCount} violations`);

    body = printViewCounts(body, report, 'violations');

    if (violationCount !== 0) {
      const list = '<ol>';
      Object.entries(report.violations).forEach(([view, violations]) => {
        violations.forEach(printViolation(list, view));
      });
      body += `${list}</ol>`;
    }
  });

  try {
    body += `${marked(`## Summary of Passing Tests: ${Object.entries(reports).reduce((total, [url, report]) => total + countPasses(report), 0)} total passing tests`)}</div></div>`;
  } catch (err) {
    console.log('Error:', err);
    body += marked('## Summary of Passing Tests');
  }

  Object.entries(reports).forEach(([url, report]) => {
    // console.log(report);
    const passesCount = countPasses(report);

    body += marked(`### ${escape(url)} ${passesCount} passes`);

    body = printViewCounts(body, report, 'passes');

    if (passesCount !== 0) {
      Object.entries(report.passes).forEach(([view, passes]) => {
        const list = `<br/><ol><br/>${passes.reduce((list, {
          description,
          nodes
        }) => {
          list += `<li>${escape(description)}<br />`;
          list += `<span>${view.toUpperCase()} Affected Nodes: <span><ul>`;
          list += nodes.reduce(writeNodeMessages, '');
          return list += '</ul></li>';
        })}`;
        body += `${list}</ol>`;
      }, '');
    }
  });
  body += '</div></div>';

  const foot = '<div></body><script></script></html>';

  fs.writeFile(file, head + body + foot);
}

var asyncGenerator = function () {
  function AwaitValue(value) {
    this.value = value;
  }

  function AsyncGenerator(gen) {
    var front, back;

    function send(key, arg) {
      return new Promise(function (resolve, reject) {
        var request = {
          key: key,
          arg: arg,
          resolve: resolve,
          reject: reject,
          next: null
        };

        if (back) {
          back = back.next = request;
        } else {
          front = back = request;
          resume(key, arg);
        }
      });
    }

    function resume(key, arg) {
      try {
        var result = gen[key](arg);
        var value = result.value;

        if (value instanceof AwaitValue) {
          Promise.resolve(value.value).then(function (arg) {
            resume("next", arg);
          }, function (arg) {
            resume("throw", arg);
          });
        } else {
          settle(result.done ? "return" : "normal", result.value);
        }
      } catch (err) {
        settle("throw", err);
      }
    }

    function settle(type, value) {
      switch (type) {
        case "return":
          front.resolve({
            value: value,
            done: true
          });
          break;

        case "throw":
          front.reject(value);
          break;

        default:
          front.resolve({
            value: value,
            done: false
          });
          break;
      }

      front = front.next;

      if (front) {
        resume(front.key, front.arg);
      } else {
        back = null;
      }
    }

    this._invoke = send;

    if (typeof gen.return !== "function") {
      this.return = undefined;
    }
  }

  if (typeof Symbol === "function" && Symbol.asyncIterator) {
    AsyncGenerator.prototype[Symbol.asyncIterator] = function () {
      return this;
    };
  }

  AsyncGenerator.prototype.next = function (arg) {
    return this._invoke("next", arg);
  };

  AsyncGenerator.prototype.throw = function (arg) {
    return this._invoke("throw", arg);
  };

  AsyncGenerator.prototype.return = function (arg) {
    return this._invoke("return", arg);
  };

  return {
    wrap: function (fn) {
      return function () {
        return new AsyncGenerator(fn.apply(this, arguments));
      };
    },
    await: function (value) {
      return new AwaitValue(value);
    }
  };
}();



var asyncToGenerator = function (fn) {
  return function () {
    var gen = fn.apply(this, arguments);
    return new Promise(function (resolve, reject) {
      function step(key, arg) {
        try {
          var info = gen[key](arg);
          var value = info.value;
        } catch (error) {
          reject(error);
          return;
        }

        if (info.done) {
          resolve(value);
        } else {
          return Promise.resolve(value).then(function (value) {
            step("next", value);
          }, function (err) {
            step("throw", err);
          });
        }
      }

      return step("next");
    });
  };
};

/**
 * @fileOverview Basic webcrawler function -- uses sets to build a queue of
 * links to be returned
 *
 * @name crawler.js
 * @author Tyler Collins
 * @license MIT
 */
/**
 * getHref -- helper function to pull the href attribute off a DOM
 * node
 *
 * @param {DOMNode[]} links
 * @returns {string|null}
 */
function getHref(links) {
  return key => {
    if (links[key].attribs) {
      return links[key].attribs.href;
    }
    return null;
  };
}

/**
 * queueLinks - parses page content and appends all links on the page to existing queue.
 *
 * @param {Promise} pageContent axios response containing page data to be
 * parsed
 * @param {Function} filterFn function to be used to filter out urls (e.g.
 * removeMedia, noFTP, etc.)
 */
function queueLinks(pageContent, filterFn = x => true) {
  if (pageContent.status === 200) {
    const links = cheerio.load(pageContent.data)('a');

    return new Set(Object.keys(links).map(getHref(links)).filter(url => typeof url === 'string').filter(filterFn).map(url => url.replace(/^https/, 'http')));
  }
  console.log(Error(`Website returned an error: ${pageContent.status}`));
  return new Set();
}

/**
 * combineLinkSets - helper function that reduces an array of sets to a single
 * set.
 *
 * @param {Set<string>} urlList
 * @param {Set<string>} urlSet
 * @returns {Set<string>}
 */
function combineLinkSets(urlList, urlSet) {
  if (urlList instanceof Set) {
    for (const address of urlList) {
      urlSet.add(address);
    }
    return urlSet;
  }
  return urlSet;
}

/**
 * crawl - Crawls through page links and builds a set of all pages to test.
 * Goes 5 levels deep through links checking for new pages by default
 *
 * @param {String} domain domain to crawl through
 * @param {Number} depth Levels to recurse through website to find new links.
 * @param {Function} filterFn function to be used to filter out urls (e.g.
 * removeMedia, noFTP, etc.)
 * @returns {Set<String>} queue of all unique links matching filterFn
 */
var crawl = (() => {
  var _ref = asyncToGenerator(function* (domain, depth = 5, filterFn) {
    // Validate url and throw error if invalid
    const url = `http://${domain}`;
    if (!validator.isURL(url)) {
      throw new Error(`Invalid url: ${url}`);
    }

    // Return initial url if depth === 0
    if (depth === 0) {
      return new Set([url]);
    }

    // Scrape main url
    const mainPage = yield axios.get(url);

    const allLinks = yield queueLinks(mainPage, filterFn);
    let links = new Set([...allLinks]);

    for (let i = 1; i < depth; i += 1) {
      const linkedContent = yield Promise.all([...links].map(axios.get));

      links = linkedContent.map(function (newPage) {
        return queueLinks(newPage, filterFn);
      }).reduce(combineLinkSets, new Set([])).difference(allLinks);
      if (links.size === 0) {
        i = depth;
      }

      for (const address of links) {
        allLinks.add(address);
      }
    }
    return allLinks;
  });

  function crawl(_x) {
    return _ref.apply(this, arguments);
  }

  return crawl;
})();

/**
 * @fileOverview Read config options
 * @name config.js
 * @author Tyler Collins
 * @license MIT
 */

const fs$1 = require('fs');
const minimist = require('minimist');

const DEFAULT_FILE = '.axecrawlerrc';

const DEFAULT_OPTS = {
  depth: 5,
  check: undefined, // undefined => check all
  output: 'reports',
  viewPorts: [{
    name: 'mobile',
    width: 360,
    height: 640
  }, {
    name: 'tablet_vertical',
    width: 768,
    height: 1024
  }, {
    name: 'tablet_horizontal',
    width: 1024,
    height: 768
  }, {
    name: 'desktop',
    width: 1440,
    height: 900
  }]
};

/**
 * parseViewPortsArg - uses a regex to parse a cmd line argument giving custom
 * viewPorts to be tested
 *
 * @param {string} views value from cmd line option --viewPorts mobile:360x640,
 * tablet:768x1024 for example.
 * @returns {Object[]} array of viewPort objects to be added to globalOptions
 */
function parseViewPortsArg(views) {
  return views.split(',').map(view => {
    const parser = /(\w+):(\d+)x(\d+)/;
    try {
      const [, name, width, height] = parser.exec(view);
      return { name, width, height };
    } catch (err) {
      throw new Error('Invalid viewports: ', views);
    }
  }).filter(view => view);
}

/**
 * crawlerOpts - combine default options, config file options, and cmd line
 * options into single global options object
 *
 * @export
 * @param {string} file filename of config file
 * @returns {Object} globalOptions object
 */
function crawlerOpts(file) {
  const argv = minimist(process.argv.slice(2));
  argv.domains = argv._;
  if (argv.viewPorts) {
    argv.viewPorts = parseViewPortsArg(argv.viewPorts);
    if (argv.viewPorts.length === 0) {
      delete argv.viewPorts;
    }
  }

  const optsFile = file || DEFAULT_FILE;
  try {
    return Object.assign(DEFAULT_OPTS, JSON.parse(fs$1.readFileSync(optsFile)), argv);
  } catch (err) {
    if (err.code === 'ENOENT' && err.path === optsFile) {
      return Object.assign(DEFAULT_OPTS, argv);
    }
    throw err;
  }
}

/**
 * testPage - runs axe-core tests for supplied testCase.  Returns the results
 *  of that test.
 *
 * @param {Object} testCase
 * @param {string} testCase.url url of the testCase
 * @param {Object} testCase.viewPort
 * @param {string} testCase.viewPort.name name of this viewPort (e.g. mobile or
 *  desktop)
 * @param {number} testCase.viewPort.width
 * @param {number} testCase.viewPort.height
 */
let testPage = (() => {
  var _ref = asyncToGenerator(function* (testCase) {
    const { url, viewPort: { name, width, height } } = testCase;
    const options = new chromeDriver.Options();
    options.addArguments('headless', 'disable-gpu', `--window-size=${width},${height}`);
    const driver = new webDriver.Builder().forBrowser('chrome').setChromeOptions(options).build();
    let outputReport = null;
    yield driver.get(url).then(function () {
      console.log('Testing: ', url, name);
      axeBuilder(driver).analyze(function (results) {
        outputReport = results;
      });
    }).then(function () {
      return driver.close();
    });
    return {
      result: outputReport,
      viewPort: testCase.viewPort
    };
  });

  return function testPage(_x) {
    return _ref.apply(this, arguments);
  };
})();

/**
 * main - main function to start scraping the website, build the queue of
 *  individual pages and run axe tests on each page
 *
 * @param {string} url homepage of website to be scraped and tested.
 */
let main = (() => {
  var _ref2 = asyncToGenerator(function* () {
    // Read config
    const opts = crawlerOpts();
    const domain = opts.domains.pop();

    // Create Queue of links on main page
    console.log(`Crawling ${domain} to depth of:  ${opts.depth}`);
    const linkQueue = yield crawl(domain, opts.depth, filterLinks(domain));

    console.log(`Found ${linkQueue.size} links within ${domain}`);
    console.log('Total urls to test:', Math.min(opts.check, linkQueue.size));
    console.log(`Testing ${opts.viewPorts.length} views: `);
    opts.viewPorts.forEach(function (viewPort) {
      console.log(`\t${viewPort.name}: ${viewPort.width}x${viewPort.height}`);
    });

    // Test each link
    Promise.all([...linkQueue].reduce(createURLViewReducer(opts), []).slice(0, opts.check * opts.viewPorts.length).map(testPage)).then(generateReportSaveFn(opts)).catch(console.log);
  });

  return function main() {
    return _ref2.apply(this, arguments);
  };
})();

/**
 * resultsToReports - function applied by Array.prototype.reduce to array of results to combine for
 *                    printing to reports
 *
 * @param {Object} reports
 * @param {Object} result
 * @param {Object} viewPort
 * @returns {Object}
 */
function resultsToReports(reports, { result, viewPort }) {
  try {
    /* eslint-disable no-param-reassign */
    reports[result.url] = Object.assign({
      violations: {},
      passes: {}
    }, reports[result.url]);

    reports[result.url].violations[viewPort.name] = result.violations;
    reports[result.url].passes[viewPort.name] = result.passes;

    /* eslint-enable no-param-reassign */
  } catch (err) {
    console.log(err);
  }
  return reports;
}

/**
 * generateReportSaveFn - output the results of axe-core's test to HTML and
 * JSON formats
 *
 * @param {Object} globalOptions
 * @returns {Function} callback function to print results of axe-core tests.
 */
function generateReportSaveFn({ output }) {
  return results => {
    console.log('Creating reports: ', `${output}.json`, `${output}.html`);
    const reports = results.reduce(resultsToReports, {});
    outputToJSON(`${output}.json`, reports);
    outputToHTML(`${output}.html`, reports);
  };
}

/**
 * createURLViewReducer - generates a callback function for used to reduce list
 * of urls into list of {url, viewPort} combinations
 *
 * @param {Object} globalOptions
 * @returns {Function} callback function for reduce
 */
function createURLViewReducer(globalOptions) {
  return (links, url) => {
    globalOptions.viewPorts.forEach(viewPort => {
      links.push({ url, viewPort });
    });
    return links;
  };
}

function filterLinks(domain) {
  return link => validator.isURL(link) && notMedia(link) && matchDomain(domain)(link);
}

polyfills();
main();

})));
